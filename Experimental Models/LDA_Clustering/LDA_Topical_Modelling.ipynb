{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "We wanted to further explore whether clustering methods can be used to provide recommendations. Ideally, users input their skills and the recommender system would output the jobs from the cluster closest to their input. The clustering method we will explore here is a Latent Dirichlet Allocation model, which would cluster the jobs based on their descriptions and provide the key terms used to identify each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##pip3 install spacy\n",
    "##python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xghan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\language.py:1895: UserWarning: [W123] Argument disable with value ['parser', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#initialize spacy model for pos tagging and lemmetization\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data on jobs\n",
    "df = pd.read_csv('../../Datasets/processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overview of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing the clean description\n",
    "df['lemmatized_tokens'] = df['description_clean'].map(lambda x:nlp(x))\n",
    "df['lemmatized_tokens']=df['lemmatized_tokens'].map(lambda x: \" \".join([token.lemma_ for token in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using count vectorizer to obtain term frequencies for each document in \"lemmatized_tokens\" column\n",
    "#min_df, which dictates the minimum number of times a word must occur is set at 80 as most words that occur less than 80 times were junk words.\n",
    "vectorizer = CountVectorizer(min_df = 80)\n",
    "X = vectorizer.fit_transform(df['lemmatized_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this function to check each word and its frequency.\n",
    "word_list = vectorizer.get_feature_names_out()\n",
    "count_list = X.toarray().sum(axis=0)\n",
    "word_dict = dict(zip(word_list,count_list))\n",
    "{k: v for k, v in sorted(word_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 5\n",
      "iteration: 2 of max_iter: 5\n",
      "iteration: 3 of max_iter: 5\n",
      "iteration: 4 of max_iter: 5\n",
      "iteration: 5 of max_iter: 5\n"
     ]
    }
   ],
   "source": [
    "#Cluster the job description into 20 clusters using LDA model\n",
    "#The number of clusters was determined by observing for overlaps in key terms between topics, which would indicated that number of topics was too high\n",
    "num_topics = 20\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, max_iter=5, learning_method='online',random_state=0, verbose =1).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the matrix of term frequencies into a list of probabilities for each topic\n",
    "transformed = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6243, 20)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to display the key words for each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (f\"Topic:{topic_idx}\")\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0\n",
      "solution business technology technical experience team ai project cloud work lead partner development stakeholder drive need develop role enterprise understand\n",
      "Topic:1\n",
      "risk management business financial client team market support experience investment control work skill compliance include finance credit regulatory provide global\n",
      "Topic:2\n",
      "system support operation provide experience service issue work sap management process maintenance level network perform knowledge maintain ensure incident center\n",
      "Topic:3\n",
      "project ensure process management plan manage quality cost budget planning report analysis delivery material review prepare production work forecast standard\n",
      "Topic:4\n",
      "work team world opportunity help people build service make technology company we look experience well join global product singapore new\n",
      "Topic:5\n",
      "experience design software development system application team work knowledge code good architecture develop technology technical cloud engineering web database platform\n",
      "Topic:6\n",
      "etl snowflake stay data solution preferably analytic datum team business experience requirement year good identify development application knowledge manage sql\n",
      "Topic:7\n",
      "requirement user test product project business experience testing team work design functional process development ensure stakeholder skill system specification management\n",
      "Topic:8\n",
      "status work disability gender datum process role opportunity orientation sexual value race equal protect experience religion we data age make\n",
      "Topic:9\n",
      "research job student singapore skill project candidate nus work covid health university write strong well require staff 19 conduct may\n",
      "Topic:10\n",
      "work skill report team management support assist job communication able good perform ability require requirement ensure relate datum experience project\n",
      "Topic:11\n",
      "product business marketing team market strategy drive experience digital management work strong insight ability key across customer manager partner skill\n",
      "Topic:12\n",
      "datum data experience analytic model business science use work analysis tool machine learning skill python team develop strong sql solution\n",
      "Topic:13\n",
      "customer sale product service account work new relationship pm company develop client experience day provide job requirement technical year maintain\n",
      "Topic:14\n",
      "security service hr information chain supply client healthcare cyber skill work opportunity professional network technology threat business project singapore strong\n",
      "Topic:15\n",
      "group bank banking technology business delivery operation process infrastructure support environment advantage recognise manage dynamic competitive function benefit innovation offer\n",
      "Topic:16\n",
      "engineering electrical electronic industrial equipment control logic engineer robotic manufacturing commission food site mechanical 3d job automation computer design system\n",
      "Topic:17\n",
      "tiktok team product inspire creativity mission user experience platform include video bring create global joy lead commit bytedance qualification responsibility\n",
      "Topic:18\n",
      "candidate please ea com resume apply datum job pte ltd shortlist personal application send we privacy email policy sg experience\n",
      "Topic:19\n",
      "computer software development experience machine learning application system science engineering job develop requirement ai vision degree knowledge programming relate design\n"
     ]
    }
   ],
   "source": [
    "#display the top 20 terms for each cluster\n",
    "display_topics(lda,vectorizer.get_feature_names_out(),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce a column denoting which cluster each job description belongs to\n",
    "arg_max = [np.where(i==max(i))[0][0] for i in transformed]\n",
    "df['cluster'] = arg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12    1050\n",
       "5      744\n",
       "4      741\n",
       "1      730\n",
       "10     496\n",
       "11     453\n",
       "18     354\n",
       "19     284\n",
       "9      280\n",
       "17     240\n",
       "7      211\n",
       "2      184\n",
       "0      125\n",
       "13      97\n",
       "8       80\n",
       "3       77\n",
       "14      59\n",
       "15      20\n",
       "16      18\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the distribution of the clusters\n",
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering jobs by clusters to determine a hypothesized cluster (together with their key words)\n",
    "df[df['cluster']==19].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesized Clusters\n",
    "<li>0: Unknown </li>\n",
    "<li>1: Banking</li>\n",
    "<li>2: IT </li>\n",
    "<li>3: Production</li>\n",
    "<li>4: Unknown</li>\n",
    "<li>5: Software Developer</li>\n",
    "<li>6: Unknown</li>\n",
    "<li>7: Unknown</li>\n",
    "<li>8: Unknown</li>\n",
    "<li>9: Unknown</li>\n",
    "<li>10: Unknown</li>\n",
    "<li>11: Product Manager</li>\n",
    "<li>12: Data Science</li>\n",
    "<li>13: Sales</li>\n",
    "<li>14: Cybsersecurity </li>\n",
    "<li>15: DBS </li>\n",
    "<li>16: Electrical Engineer </li>\n",
    "<li>17: Tiktok/ByteDance</li>\n",
    "<li>18: Unknown</li>\n",
    "<li>19: AI/NLP roles</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "To conclude, we realised that most of the key terms for each topic were not skills, but rather other parts of the job description such as company information or day-to-day roles. Therefore, inputting skills to determine the closest cluster would yield very poor results. If there were methods to sieve out skills from these job descriptions, it would greatly improve the viability of such a recommender system. Furthermore, some clusters were difficult to identify. Nonetheless, we realise that this model can provide a useful function of generating key terms for each of these clusters (which might be industry or job title), which might help with users who are crafting resumes, as resumes with key terms tend to get through automated systems better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining good clusters with well defined key terms\n",
    "clusters = [\"banking\", \"it\",\"project manager\", \"software developer\",\"product manager\",\"data science\",\"sales\",\"cybersecurity\",\"electrical engineer\",\"ai and nlp\"]\n",
    "relevant_index =[1,2,3,5,11,12,13,14,16,19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a dataframe of job titles and key terms\n",
    "result_dict={}\n",
    "result_dict['title'] = clusters\n",
    "key_terms=[]\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    if topic_idx in relevant_index:\n",
    "        key_terms.append([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-20 - 1:-1]])\n",
    "result_dict['key_terms']=key_terms\n",
    "\n",
    "result_df = pd.DataFrame(result_dict)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"key_terms.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8b325dba0a5d88be3dbd035e7bbab64b862c49c49fbb32327f9c81875f85a34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
