{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14630297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_job(job):\n",
    "    job_arr = job.split(' ')\n",
    "    formatted_job = '-'.join(job_arr)\n",
    "    return formatted_job\n",
    "\n",
    "def get_all_soup(job_keyword):\n",
    "    soup_list = []\n",
    "    \n",
    "    # scrapping the very first page\n",
    "    job = format_job(job_keyword)\n",
    "    url = f'https://www.jobstreet.com.sg/en/job-search/{job}-jobs/1/?job-type=internship'\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    print('1', r.status_code)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    soup_list.append(soup)\n",
    "    \n",
    "    # extract the number of jobs related to this job keyword, to find out the number of pages of jobs this keyword has\n",
    "    num_of_jobs_info = soup.find('span', {'class': 'sx2jih0 zcydq84u _18qlyvc0 _18qlyvc1x _18qlyvc1 _1d0g9qk4 _18qlyvc8'}).text\n",
    "    stop_words = ['of', 'jobs']\n",
    "    first = num_of_jobs_info.find(stop_words[0])\n",
    "    second = num_of_jobs_info.find(stop_words[1])\n",
    "    num_of_jobs = num_of_jobs_info[first+3: second-1]\n",
    "    num_of_pages = int(num_of_jobs)//30 + 1\n",
    "    print('num of jobs:', num_of_jobs)\n",
    "    print('num of pages:', num_of_pages)\n",
    "    \n",
    "    # get all the soup for all the pages related to this job keyword\n",
    "    for page in range(2, num_of_pages+1):\n",
    "        url1 = f'https://www.jobstreet.com.sg/en/job-search/{job}-jobs/{page}/?job-type=internship'\n",
    "        print(page, url1)\n",
    "        r1 = requests.get(url1)\n",
    "        print(page, r1.status_code)\n",
    "        soup1 = BeautifulSoup(r1.content, 'html.parser')\n",
    "        soup_list.append(soup1)\n",
    "        time.sleep(3)\n",
    "        \n",
    "    return soup_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9eb814",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.jobstreet.com.sg'\n",
    "def get_url(soup_list):\n",
    "    url_list = []\n",
    "    job_title_list = []\n",
    "    for soup in soup_list:\n",
    "        divs = soup.find_all('div', class_ = 'sx2jih0 zcydq876 zcydq866 zcydq896 zcydq886 zcydq8n zcydq856 zcydq8f6 zcydq8eu')\n",
    "        for item in divs:\n",
    "            job_title = item.find('span', class_ = 'sx2jih0').text\n",
    "            job_title_list.append(job_title)\n",
    "            job_url = item.find('a', href = True)['href']\n",
    "            job_full_url = base_url + job_url\n",
    "            url_list.append(job_full_url)\n",
    "    return (url_list, job_title_list)\n",
    "    # return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ae27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional codes to extract the extra job information\n",
    "stop_points = [\"Career Level\", \"Qualification\", \"Job Type\", \"Job Specializations\"]\n",
    "stop_point_string_lengths = [len(stop_point) for stop_point in stop_points]\n",
    "\n",
    "# extracting the relevant details given a url\n",
    "def get_job_details(url):\n",
    "    r = requests.get(url)\n",
    "    print(r.status_code)\n",
    "    s = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "    # get the job title\n",
    "    job_title = s.h1.text\n",
    "    #title = item.find('span', class_ = 'sx2jih0').text\n",
    "    \n",
    "    # get the company name\n",
    "    company = s.find('div', {'class': 'sx2jih0 zcydq86i'}).find('span').text\n",
    "    \n",
    "    # get the work location\n",
    "    location = s.find('div', {'class': 'sx2jih0 zcydq86a'}).text\n",
    "    \n",
    "    # get job information\n",
    "    job_information = s.find('div', {'class': 'sx2jih0 zcydq86q zcydq86v zcydq86w'}).text\n",
    "    job_information_index_to_slice = job_information.find('Additional Information')\n",
    "    job_information = job_information[15:job_information_index_to_slice] #start from 15 to slice out 'Job Information' \n",
    "    ## HERE MIGHT HAVE ERROR, IF YES, GO BACK TO job_information[:job_information_index_to_slice]\n",
    "    \n",
    "    # get sector/indsutry\n",
    "    company_extra_info = s.find_all('div', {'class': 'sx2jih0 _17fduda0 _17fduda7'})[1].text\n",
    "    first = company_extra_info.find('Industry')\n",
    "    second = company_extra_info.find('Benefit')\n",
    "    sector = company_extra_info[first+8:second]\n",
    "    \n",
    "    # get additional information - career level, qualification, years of experience, job type and specialisation\n",
    "    jai = s.find('div', {'class': 'sx2jih0 _17fduda0 _17fduda7'}).text\n",
    "    indices = [jai.find(stop_points[i]) for i in range(len(stop_points))]\n",
    "    jai_results = {}\n",
    "    for i, stop_point in enumerate(stop_points):\n",
    "        if i == len(stop_points)-1:\n",
    "            extract = jai[indices[i] + stop_point_string_lengths[i] :]\n",
    "        else:\n",
    "            extract = jai[indices[i] + stop_point_string_lengths[i]: indices[i+1]]\n",
    "        jai_results[stop_point] = extract\n",
    "    \n",
    "    # get salary\n",
    "    test_salary = s.find_all('div', {'class': 'sx2jih0 zcydq86a'})[1].text\n",
    "    if 'SGD' in test_salary:\n",
    "        pay_range = test_salary\n",
    "    else:\n",
    "        pay_range = ''\n",
    "    '''    \n",
    "    job = {\n",
    "        'job_title': job_title,\n",
    "        'company': company,\n",
    "        'location': location,\n",
    "        'description': job_information,\n",
    "        'level': jai_results['Career Level'],\n",
    "        'qualification': jai_results['Qualification'],\n",
    "        'job_type': jai_results['Job Type'],\n",
    "        'job_specialisation': jai_results['Job Specializations'],\n",
    "        'pay_range': pay_range\n",
    "          }\n",
    "    '''\n",
    "    job = (url, job_title, company, location, sector, jai_results['Job Type'], jai_results['Career Level'], \n",
    "           jai_results['Job Specializations'], jai_results['Qualification'], pay_range, job_information)\n",
    "    \n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = [\n",
    "    'data',\n",
    "    'tech' ,\n",
    "    'software',\n",
    "    'product manager',\n",
    "    'analyst',\n",
    "    'analytics',\n",
    "    'data scientist',\n",
    "    'data engineer',\n",
    "    'artificial intelligence',\n",
    "    'machine learning' ,\n",
    "    'quantitative',\n",
    "    'business intelligence',\n",
    "    'computer vision',\n",
    "    'natural language processing',\n",
    "    'data pipelines'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = set()\n",
    "\n",
    "def extract_jobstreet(keywords):\n",
    "    jobs_scraped = 0\n",
    "    for keyword in keywords:\n",
    "        print(f'Current Keyword is: {keyword}')\n",
    "        soups = get_all_soup(keyword)\n",
    "        url_list = get_url(soups)[0]\n",
    "        curr_job_count = 0\n",
    "        for i, url in enumerate(url_list):\n",
    "            print(i, url)\n",
    "            results.add(get_job_details(url))\n",
    "            time.sleep(5)\n",
    "            jobs_scraped += 1\n",
    "            curr_job_count += 1\n",
    "            print('Number of jobs scraped :', jobs_scraped)\n",
    "            print(keyword, \":\", curr_job_count)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e97bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_jobstreet(['business intelligence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33befec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['url', 'job_title', 'company', 'location', 'sector', 'job_type', 'level', 'job_specialisation', 'qualification', 'pay_range', 'description']\n",
    "df = pd.DataFrame(results, columns = col_list)\n",
    "# df.head()\n",
    "df.to_csv('jobstreet.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
